---
title: "pre_lab_12.Rmd"
author: "Derek Willis"
date: "2024-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An intro to text analysis

Throughout this course, we've been focused on finding information in structured data. We've learned a lot of techniques to do that, and we've learned how the creative mixing and matching of those skills can find new insights.

What happens when the insights are in unstructured data? Like a block of text?

Turning unstructured text into data to analyze is a whole course in and of itself -- and one worth taking if you've got the credit hours -- but some simple stuff is in the grasp of basic data analysis.

To do this, we'll need a new library -- [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html), which you can guess by the name plays very nicely with the tidyverse. So install it in the console with `install.packages("tidytext")` and we'll get rolling.

### Task 1: Load libraries and settings

**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use.

```{r}
install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
library(rvest)
```

Remember the end of the previous lab, where you scraped all of those press release links from Maryland Sen. Ben Cardin's website? We're going to take that one step further and analyze the actual text of those releases using tidytext. Our starting question: what words or phrases appear most in Cardin's press releases?

To answer this question, we'll use the text of those releases. For the scraping lab we gathered the URLs of 999 of Cardin's press releases, and for this exercise we'll be working with the text of them.

Let's read in this data and examine it:

### Task 2: Read in data

**Task** Run the following code and describe the dataframe it outputs. **Answer** It is a list of press releases made by Ben Cardin. We have the date, url, title and the first paragraph (text) from the press release. 

```{r}
releases <- read_rds("data/cardin_releases.rds")
```

As an aside, below is an example of how you would scrape the text from the first 10 press releases. We already have the text in the dataframe, but this lets you know how it was collected.

### Task 3: Example of gathering text

**Task** Run the following code to gather the text for the first 10 press releases.

```{r}

urls <- releases |> top_n(10) |> pull(url)

release_text <- tibble(url = character(), text = character())

# loop over each url in the list of urls
for (u in urls){
  # wait a fraction of a second so we don't hammer the server
  Sys.sleep(0.2)
  # read in the html from the url
  html <- u |> read_html()
  # use the xpath of the text of the release to grab it and call html_text() on it
  text <- html |> 
    html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |> 
    html_text()
  
  release_text <- release_text |> add_row(url = u, text = str_squish(text))
}

release_text
```

What we want to do is to make the `text` column easier to analyze. Let's say we want to find out the most commonly used words. We'll want to remove URLs from the text of the releases since they aren't actual words. Let's use mutate to make that happen:

### Task 4: Remove URLs from content

**Task** Run the following code.

```{r}
releases <- releases |>
  mutate(text = gsub("http.*","", text))
```

If you are trying to create a list of unique words, R will treat differences in capitalization as unique and also will include punctuation by default, even using its `unique` function:

### Task 5: Trying out unique

**Task** Run the following code and describe what the `unique` function does to the original list of words. **Answer** it removes any duplicates from the list. We basically get a list of all the distinct values including punctuation. It is case sensitive. 

```{r}
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
```

Fortunately, this is a solved problem with tidytext, which has a function called `unnest_tokens` that will convert the text to lowercase and remove all punctuation. The way that `unnest_tokens` works is that we tell it what we want to call the field we're creating with this breaking apart, then we tell it what we're breaking apart -- what field has all the text in it. For us, that's the `text` column:

### Task 6: Trying out unnest_tokens

**Task** Run the following code and describe the output of using the `unnest_tokens` function. **Answer** We get a list of all words from the 'text' column in 'releases' dataframe. The first argument of unnest_tokens takes in the name of the output column to be created and the second argument takes the name of the input column that has to be split, which is 'text' here. Since we haven't passed a third argument, the default unit of tokenizing is 'word' but otherwise we could have also extracted sentences by adding a third argument, token = "sentences"
```{r}
unique_words <- releases |> select(text) |>
  unnest_tokens(word, text)
View(unique_words)
```

### Task 7: Make a column chart

**Task** Run the following code and describe what the resulting graphic shows. Is it interesting? **Answer** The resulting graphic shows a bar chart of the top 25 most common unique words in the dataset. No, it's not interesting because the most common words are conjunctions, prepositions, and articles like 'the', 'and', 'of' etc which don't say anything about our data.

Now we can look at the top words in this dataset. Let's limit ourselves to making a plot of the top 25 words, and we'll use the function `count` to do the counting:

```{r}
unique_words |>
  count(word, sort = TRUE) |>
  top_n(25) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in Cardin releases")
```

Well, that's a bit underwhelming - a lot of very common (and short) words. This also is a solved problem in working with text data, and words like "a" and "the" are known as "stop words". In most cases you'll want to remove them from your analysis since they are so common. Tidytext provides a dataframe of them that we'll load, and then we'll add some of our own.

### Task 8: Load the stop words

**Task** Run the following code

```{r}
data("stop_words")

stop_words <- stop_words |> 
  add_row(word = "ben") |> 
  add_row(word = "cardin") |> 
  add_row(word = "senator") |>
  add_row(word = "senators") |>
  add_row(word = "maryland") |>
  add_row(word = 'federal') |> 
  add_row(word = 'u.s') |> 
  add_row(word = 'md') |> 
  add_row(word = 'senate') |> 
  add_row(word = "hollen") |> 
  add_row(word = "van") |> 
  add_row(word = "chris") |> 
  add_row(word = "project") |> 
  add_row(word = "program") 

```

Then we're going to use a function we haven't used yet called an `anti_join`, which filters out any matches. So we'll `anti_join` the stop words and get a list of words that aren't stop words.

From there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.

### Task 9: Using anti_join

**Task** Run the following code and describe the results. Is it more interesting than before?

**Answer** This code first takes our list of unique words and through the anti_join function removes any words that appear in the stop word dictionary + the list of words we added above to the dictionary. The result contains only words that are not in stop_words. I believe the tally function is similar to count function and it returns the number of occurrences of the word (n). sort= TRUE sorts the list in descending order. We also get a column which is a percentage of  how often a word appears. Finally we only see the top 10 most frequent words. The value of the percentage here does not matter as much as the distance between two percentage values. It is more interesting than before because now we dont have filler words that tell us nothing. 

```{r}
unique_words |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Those seem like more relevant unique words. Now, here's where we can start to do more interesting and meaningful analysis. Let's create two dataframes of unique words based on time: one for all of 2022 and the other for all of 2024:

### Task 10: Create dataframes for 2022 and 2024

**Task** Run the following code

```{r}
unique_words_2022 <- releases |>
  filter(year(date) == 2022) |>
  select(text) |>
  unnest_tokens(word, text)

unique_words_2024 <- releases |>
  filter(year(date) == 2024) |>
  select(text) |>
  unnest_tokens(word, text)
```

Then we can create top 10 lists for both of them and compare:

### Task 11: Create dataframes with the top 10 words in each year

**Task** Run the following code and describe the results. **Answer** unique_words_2022 and unique_words_2024 is a list of top 10 most common unique words in the year 2022 and 2024 respectively minus all the stop words. It's the same as before, we group all the words, count the number of their occurrences and find a percentage of how often they occurred. So, here we have two dataframes, one for the year 2022 and one dataframe for 2024. "health" "support" and "community were the top 3 words in 2022 and "baltimore" "support" and "funding were the top 3 words in 2024. 

```{r}
unique_words_2022 |>
  anti_join(stop_words) |> 
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

unique_words_2024 |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

In the 2022 top 10 list, "health" is first, which makes some sense, while the 2024 list leads with "baltimore".

## Going beyond a single word

The next step in text analysis is using `ngrams`. An `ngram` is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.

The code to make ngrams is similar to what we did above, but involves some more twists.

So this block is is going to do the following:

1.  Use the releases data we created above, and filter for 2022 releases.
2.  Unnest the tokens again, but instead we're going to create a field called bigram, break apart summary, but we're going to specify the tokens in this case are ngrams of 2.
3.  We're going to make things easier to read and split bigrams into word1 and word2.
4.  We're going to filter out stopwords again, but this time we're going to do it in both word1 and word2 using a slightly different filtering method.
5.  Because of some weirdness in calculating the percentage, we're going to put bigram back together again, now that the stop words are gone.
6.  We'll then group by, count and create a percent just like we did above.
7.  We'll then use top_n to give us the top 10 bigrams.

### Task 12: Create a dataframe with the top 10 two-word phrases for 2022

**Task** Run the following code and describe the results. **Answer** The result is a list of bi-grams from the year 2022 specifically which dont have any stop words in them. First, all the rows with stop words were removed, so we have a list of bi-grams, followed by the count of how many times they occurred together and a percentage of how often they occurred. "amount included" was the top bi-gram in 2022 followed by "health care" and "prince george's" 

```{r}
releases |>
  filter(year(date) == 2022) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |> #is the first word in stop words? if yes, get rid of that
  filter(!word2 %in% stop_words$word) |> #is the second  word in stop words? if yes, get rid of that
  mutate(bigram = paste(word1, word2, sep=" ")) |> 
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

And we already have a different, more nuanced result. Health was among the top single words, and we can see that "health care", "human rights" and "chesapeake bay" are among the top 2-word phrases. What about 2024?

### Task 13: Create a dataframe with the top 10 two-word phrases for 2024

**Task** Run the following code and describe the results. **Answer** The result is a list of bi-grams from the year 2024 specifically which dont have any stop words in them. First, all the rows with stop words were removed, so we have a list of bi-grams, followed by the count of how many times they occurred together and a percentage of how often they occurred. "health care" was the top bi-gram in 2024 followed by "amount awarded" and "purpose funds" This adds up because a senator would want to talk about the funding they have gotten for the state. 

```{r}
releases |>
  filter(year(date) == 2024) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

No more "covid 19", for one, but not a lot of changes otherwise. You'll notice that the percentages are very small; that's not irrelevant but in some cases it's the differences in patterns that's more important.

There are some potential challenges to doing an analysis. For one, there are variations of words that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context.

Or..... we could have R tackle this.

### Task 14: Install a package for stemming and lemmatizing words

**Task** Run the following code.

```{r}
#install.packages("pacman") # comment this line out after you run it the first time
pacman::p_load_gh("trinker/textstem")
library(textstem)
```

[textstem](https://github.com/trinker/textstem) is a library that has two functions: stemming and lemmatization. What are they? Both of them convert a set of words or phrases into different forms, but the differences are worth seeing:

### Task 15: Stemming vs. Lemmatization

**Task** Run the following code and describe the differences between the two results. Which one might be more useful for the press release data? **Answer** Both functions convert words into different forms. Stem seems to reduce the word to its base word. Lemmatizing seems to take into the context in which the word was used. Lemmatize is more concept driven because all words barring "driver" (noun) are about the act (verb) of driving. Stemming is more specific and retains the tense of the verb. Lemmatizing is more useful for the press release data because we want to understand the overlying themes of the data, such as grouping words like "funding" and "funds" into "fund" 

```{r}
dw <- c('driver', 'drive', 'drove', 'driven', 'drives', 'driving')

stem_words(dw)

lemmatize_words(dw)

```

Let's try lemmatization on the 2024 unique words, and then compare the top ten to the original top ten from Task 11:

### Task 16: Lemmatization vs. Words

**Task** Run the following code and describe the differences between the two results. Which one is more useful? **Answer**
The first data frame is lemmatized and therefore words such as "funding" and 'funds' are no longer separate. They are grouped as one under "fund" I am guessing words such as "communities" and "community" got grouped as well to emerge as the second most common word. The top word is "fund" which adds up because the senator would want to talk about money. 
The second data frame has no new processing done on it and shows the top 10 most common unique words from 2024. The top word is "baltimore" followed by "support" and "funding" Here we have a separate row for "fund"
The first dataframe is more useful because we can gather the most common emergent theme if word meanings are allowed to coalesce.
This would be good for finding divergent themes across demographics whether Ben Cardin speaks differently to different sects.
```{r}
unique_words_2024_lemma <- lemmatize_words(unique_words_2024$word) |> as_tibble() |> rename(word = value)

unique_words_2024_lemma |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

unique_words_2024 |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

```
## Sentiment Analysis

Another popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we've already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called `get_sentiments`. The most common dataframe is called "bing" which has nothing to do with the Microsoft search engine. Let's load it:

### Task 17: Load the bing lexicon and produce sentiments for our 2022 and 2024 unique words

**Task** Run the following code and describe the results. Do any of the sentiments seem incorrect or counter-intuitive? **Answer** The code gives us a list of unique words from 2022 and 2024 that are in the bing dictionary and tells us whether it is a positive or a negative word. Whether it is a positive or negative word is defined in the "bing" dictionary. We also get a count og how often that word occurred. "critical" is counter intuitive because it isn't necessarily true the word was used negatively. It could be referring to critical infrastructure. "issues" is also stated as "negative" but the sentence could have mentioned that "issues were mitigated/solved" which would mean it's a positive thing that happened. We should take into account the context in which the word was used.
```{r}
bing <- get_sentiments("bing") #dictionary of words and their positive/negative connotations

bing_word_counts_2022 <- unique_words_2022 |>
  inner_join(bing) |>
  count(word, sentiment, sort = TRUE)

bing_word_counts_2024 <- unique_words_2024 |>
  inner_join(bing) |>
  count(word, sentiment, sort = TRUE)

View(bing_word_counts_2022)
View(bing_word_counts_2024)
```

Gauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data.
